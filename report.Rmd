---
title: "The Fishy Case of Seafood Fraud"
author: "Elizabeth Serrano"
date: "2022-12-12"
output: html_document
---
# Introduction

  At first glance, It may be odd to hear the words “fish” and “fraud” in the same sentence but in actuality, fish fraud is a growing issue in the seafood industry that impacts everyone, even you. Fish fraud, also known as Seafood fraud, is the practice of misleading customers about the seafood they are purchasing, typically to increase profits. Fish fraud can be intentional, such as  branding a cheaper fish as a more expensive fish for the sake of making more money, or can be done accidentally through a variety of ways ranging from placing the label on the wrong seafood to translation issues along the shipping route. 

Some common ways that fish fraud can fly under the radar include: 

  - Packaging the seafood without the head, scales, or other identifiable body parts. This makes it easy to masquerade one seafood for another. 
  - Misbranding one fish for another fish that has a similar texture, color, and/or flavor. 
  - Incorporate the seafood with other ingredients to disguise the fraud. This can often be found at restaurants or other dining locations. 

  
  Multiple sources have mentioned the frequency of fish fraud from country to country and on a global level, however, the percentages sometimes vary. This can be attributed to difficulties in detecting instances of fish fraud, under reporting fish fraud, or even determining if a given circumstance counts as fish fraud in the first place. While the numbers may be disputed, there’s no denying that fish fraud is a widespread issue that most people don’t seem to be aware of.  The goal of this project is to raise consumer awareness on the topic of fish fraud and help consumers make more informed decisions about the seafood they are purchasing. 
  The questions of interest are: 
  
  - How common is fish fraud?
  - What are some of the consequences or negative impacts of fish fraud?
  
  To answer these questions, I will be using the “Global Fraud List for 2018” data set that was provided to me by a senior scientist of Oceana, an organization dedicated to protecting the world’s oceans and is also a vocal advocate against the practice of fish fraud within the seafood industry. I originally received this data set in the form of an excel document that contained meta-data of 91 studies examining cases of fish fraud/mislabeling ranging from 1915 to 2017. Given the messy nature of the dataset, this project will involve a lot of cleaning. I will also be creating a lot of data frames with the necessary variables to determine the total percentage of mislabeled seafood samples, the states where most seafood samples were acquired from, the percentage of seafood samples that actually match the name on their label, and many more. 

# Data Preparation

```{r setup, include=TRUE}
 knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) # has many data packages to tidy data
library(rmarkdown) # to create a dynamic document in R
library(knitr) #to generate rmarkdown files
library(dplyr) #to manipulate my data
library(tidyr) #to clean up messy data
library(ggplot2) #to display my data in graphs 
library(treemapify) #to convert my data to be used in a treemap
library(treemap) # to make a treemap
library(usmap) #to create a U.S. map to determine where most of the seafood samples came from 

#Read in the data to be used 
FishFraud <- read.csv("Fish Fraud Data CSV.csv")
```

As previously mentioned, the raw data set was originally an excel spreadsheet but after turning it into a CSV file, I was able to turn it into a data frame called “FishFraud.”

While the FishFraud data frame contained a lot of variables, I found that not all of them were necessary. So I decided to remove them and create a new data frame called “TidyFishFraud”. I decided to do this because the removed variables either contained information that was irrelevant to my purposes or there wasn’t enough data in the columns to do anything meaningful with it. Additionally, since the data was originally an excel spreadsheet, all of the empty rows that came after the data table were added to the CSV file as NAs. So I decided to remove those. I also had to replace a single cell in the 943rd row with the numerical value of 22 when it was previously entered as “22 actual”. I did this because all the other data in that column were digits and I wanted to keep it consistent. 

```{r}
TidyFishFraud <- select(FishFraud, -Fraud.map.category, -Notes, -repeat., -Species.Geographic.Range, -Amount.of.processing, -Website, -Reference, -In.fraud.map., -In.endnote., -Fraud.Map.ID., -Potentially.Farmed., -farmed.n, -X, -X.1, -X.2, -X.3, -X.4, -X.5, -X.6, -X.7, -X.8) 

TidyFishFraud <- filter(
  TidyFishFraud, 
  !is.na(Study.number)
)

TidyFishFraud[943,3] <- 22

```

We now have a cleaner version of the data I will be using today. Let’s take a quick glimpse at it. 

```{r}
glimpse(TidyFishFraud)
```

Let’s take a moment to go over the significance of some of these variables. 
I decided to keep “Study.number” and “Study.authors..year” variables as ways for me to identify the studies used as I work with the data. In the event that one of these two columns was lost throughout the data manipulation process, I still had the other column to help me identify the study I was looking at. The “Number.of.samples” variable shows the total number of samples per study conducted. “Number.of.samples” and “Number.of.mislabeled.samples” are what the name suggests. “Fraction.fish.mislabled” is the number of mislabeled samples divided by the total number of samples. “Sampling.Area…where.retail.fish.purchased.” is the geographical location where the samples came from. “Fish.name.sortable” is the common name for the seafood that is being mentioned on the packaging, while the “Label” variable is the name of the fish as seen on the package’s label. “Actual…Latin” tells us the scientific name of the actual fish, regardless of what’s on the label and, “Actual…Common” tells us the common name for that fish. “Species.group” mentions the seafood type of the packages seafood with no regard to what’s on the label. “Point.in.supply.chain” references where along the supply chain the samples came from. The “Health.Risk” variable tells us the health risks associated with consuming the packaged seafood. The “IUCN.Conservation.Status.” variable tells us the conservation status of the actual seafood. “Fraud.mislabeleing” tells us the confirmed cases of fish fraud. 

This is a brief summary of these variables but I will go more in-depth with them throughout this report. 

# Exploratory Data Analysis

The first question I wanted to address was, location-wise, where were most of the seafood samples acquired from? To address this, I created a new data frame called “Fraud Location” with the study number/authors and the sampling area which the samples were acquired from.   
```{r}
FraudLocation <-select(TidyFishFraud, Study.number, Study.authors...year, Sampling.Area..where.retail.fish.purchased.)

```

From here, I had to do a lot of cleaning because the data was entered in a very inconsistent manner. As seen below: 

```{r}
#Purchased within US but exact state is undetermined. 
FraudLocation$Sampling.Area..where.retail.fish.purchased.[FraudLocation$Sampling.Area..where.retail.fish.purchased.== "USA"] <- "USA (state undetermined)"

FraudLocation$Sampling.Area..where.retail.fish.purchased.[FraudLocation$Sampling.Area..where.retail.fish.purchased.== "US"] <- "USA (state undetermined)"

FraudLocation$Sampling.Area..where.retail.fish.purchased.[FraudLocation$Sampling.Area..where.retail.fish.purchased.== "21 states"] <- "USA (state undetermined)"

FraudLocation$Sampling.Area..where.retail.fish.purchased.[FraudLocation$Sampling.Area..where.retail.fish.purchased.== "United States"] <- "USA (state undetermined)"

FraudLocation$Sampling.Area..where.retail.fish.purchased.[FraudLocation$Sampling.Area..where.retail.fish.purchased.== "medium size city in SE US"] <- "USA (state undetermined)"
```

Sometimes, there were some samples that did not reference a location. These were empty strings that I wanted to turn into an “Undetermined” variable.

```{r}
#There are some samples that don't have any location mentioned. 
#These are empty strings and I will replace them with NA. 
FraudLocation[FraudLocation == ''] <- NA

#Replace the NA's with an Undetermined Location 
FraudLocation[is.na(FraudLocation)] <- "Undetermined"
```

Most of the work here involved renaming locations to represent only the state where the samples came from. 

```{r}
#Replace the locations to show the state where the samples were retrieved from 

#New York
FraudLocation[FraudLocation == "NY"] <- "New York"
FraudLocation[FraudLocation == "New York City "] <- "New York"
FraudLocation[FraudLocation == "New York City"] <- "New York"
FraudLocation[FraudLocation == "New Bedford, MA (company)"] <- "New York"
FraudLocation[FraudLocation == "NYC"] <- "New York"
FraudLocation[FraudLocation == "New York, New York"] <- "New York"

#Massachusetts
FraudLocation[FraudLocation == "Boston, MA"] <- "Massachusetts"
FraudLocation[FraudLocation == "MA (company)"] <- "Massachusetts"
FraudLocation[FraudLocation == "MA (company)"] <- "Massachusetts"

#Washington 
FraudLocation[FraudLocation == "W.Washington State"] <- "Washington"
FraudLocation[FraudLocation == "Seattle, WA"] <- "Washington"
FraudLocation[FraudLocation == "Bellingham, WA"] <- "Washington"
FraudLocation[FraudLocation == "Washington, DC "] <- "Washington"
FraudLocation[FraudLocation == "Washington, DC"] <- "Washington"

#California
FraudLocation[FraudLocation == "Los Angeles"] <- "California"
FraudLocation[FraudLocation == "Los Angeles, CA, USA"] <- "California"
FraudLocation[FraudLocation == "California, Los Angeles"] <- "California"
FraudLocation[FraudLocation == "California, San Francisco"] <- "California"
FraudLocation[FraudLocation == "California, Oakland"] <- "California"
FraudLocation[FraudLocation == "California, San Jose"] <- "California"
FraudLocation[FraudLocation == "California, Sausalito"] <- "California"
FraudLocation[FraudLocation == "California, San Rafael"] <- "California"
FraudLocation[FraudLocation == "California, Walnut Creek"] <- "California"
FraudLocation[FraudLocation == "California, Orange County"] <- "California"
FraudLocation[FraudLocation == "Santa Cruz, CA"] <- "California"
FraudLocation[FraudLocation == "San Diego"] <- "California"
FraudLocation[FraudLocation == "Santa Monica, CA (company), CA (purchaser)"] <- "California"
FraudLocation[FraudLocation == "San Francisco, CA"] <- "California"
FraudLocation[FraudLocation == "Sentencing in Los Angeles California"] <- "California"

#Florida 
FraudLocation[FraudLocation == "Florida - Jacksonville area"] <- "Florida"
FraudLocation[FraudLocation == "Tampa, FL"] <- "Florida"
FraudLocation[FraudLocation == "Tampa, St. Petersburg, FL"] <- "Florida"
FraudLocation[FraudLocation == "Panama City, FL"] <- "Florida"
FraudLocation[FraudLocation == "Hialeah, FL"] <- "Florida"
FraudLocation[FraudLocation == "Volusia county, FL"] <- "Florida"
FraudLocation[FraudLocation == "Fort Myers, Florida"] <- "Florida"
FraudLocation[FraudLocation == "Tallahassee, Florida"] <- "Florida"
FraudLocation[FraudLocation == "Tampa Bay, Florida"] <- "Florida"
FraudLocation[FraudLocation == "North Miami Beach, FL"] <- "Florida"
FraudLocation[FraudLocation == "Miami, FL (company)"] <- "Florida"
FraudLocation[FraudLocation == "Tampa, FL (company)"] <- "Florida"
FraudLocation[FraudLocation == "FL (company)"] <- "Florida"
FraudLocation[FraudLocation == "Hialeah, FL (company)"] <- "Florida"
FraudLocation[FraudLocation == "FL (purchaser)"] <- "Florida"

#Missouri 
FraudLocation[FraudLocation == "St. Louis, MO"] <- "Missouri"
FraudLocation[FraudLocation == "Kansas City, MO"] <- "Missouri"

#Georgia
FraudLocation[FraudLocation == "Atlanta, GA"] <- "Georgia"
FraudLocation[FraudLocation == "GA (company)"] <- "Georgia"
FraudLocation[FraudLocation == "Savannah, GA"] <- "Georgia"

#Maryland 
FraudLocation[FraudLocation == "Baltimore, MD"] <- "Maryland"

#Arizona 
FraudLocation[FraudLocation == "Phoenix,AZ"] <- "Arizona"

#Illinois 
FraudLocation[FraudLocation == "Chicago, IL"] <- "Illinois"
FraudLocation[FraudLocation == "Addison, IL (company)"] <- "Illinois"
FraudLocation[FraudLocation == "Oak Park, IL"] <- "Illinois"
FraudLocation[FraudLocation == "Chicago and suburbs"] <- "Illinois"

#North Carolina 
FraudLocation[FraudLocation == "Gastonia, NC"] <- "North Carolina"

#Virginia 
FraudLocation[FraudLocation == "VA (company)"] <- "Virginia"

```

There were also many cases where a single cell mentioned 2+ states. So, in order to get one state per cell, I repeated the rows of certain studies by the number of states mentioned and replaced each cell with the total amount of times each state was referenced. Each one of these cases was turned into a new data frame with a variable that references the study those samples came from. When I finished repeating all the rows, I will eliminate all the rows where 2+ states are mentioned in the FraudLocation data frame, then I will combine all the data frames together using rbind() and save it back into the FraudLocation data frame. 

```{r}

#Study 1
Study1 <-FraudLocation %>% slice(rep(1,2)) #Replicated 1st study twice
Study1[1,3] <- "USA (state undetermined)"
Study1[2,3] <- "Canada"  

#Study 3
Study3 <- FraudLocation %>% slice(rep(17,8)) #Replicated 3rd study 8 times
Study3[1,3] <-"Delaware"
Study3[2,3]<- "Florida"
Study3[3,3]<- "Illinois"
Study3[4,3]<- "Massachusetts"
Study3[5,3]<- "New York"
Study3[6,3]<- "North Carolina"
Study3[7,3]<- "South Carolina"
Study3[8,3]<- "Wisconsin"  #Each state is now represented in the 3rd study 

#Study 6
#two rows x 5 states = 10 repeats
Study6<-FraudLocation %>% slice(rep(34,10))
#Each two rows will have the same state name out of the 5 states mentioned. 
Study6[1:2, 3]<- "California"
Study6[3:4, 3]<- "New York"
Study6[5:6, 3]<- "North Carolina"
Study6[7:8, 3]<- "Missouri"
Study6[9:10, 3]<- "Florida"

#Study 9
#6 rows x 2 states = 12 repeats
Study9<-FraudLocation %>% slice(rep(59,12))
#I will repeat the same process here. 
Study9[1:6, 3]<-"Colorado"
Study9[7:12, 3]<-"New York"


#Study 10
#3 rows x 2 states = 6 repeats
Study10<-FraudLocation %>% slice(rep(65,6))
Study10[1:3,3]<- "Florida"
Study10[4:6,3]<- "Alabama"

#Study 12
#2 rows x 2 states
Study12<-FraudLocation %>% slice(rep(79,4))
Study12[1:2,3] <-"California"
Study12[3:4,3] <-"Washington"

#Study 17
#4 rows x 10 states
Study17<- FraudLocation %>% slice(rep(166,40))
Study17[1:4,3] <- "California"
Study17[5:8,3]<- "Arizona"
Study17[9:12,3]<- "Texas"
Study17[13:16,3]<- "North Carolina"
Study17[17:20,3]<- "South Carolina"
Study17[21:24,3]<- "Tennessee"
Study17[25:28,3]<- "Lousiana"
Study17[29:32,3]<- "Florida"
Study17[33:36,3]<- "Maryland"
Study17[37:40,3]<- "Massachusetts"


#Study 18
#14 rows x 3 states 
Study18<- FraudLocation %>% slice(rep(170,42))
Study18[1:14,3]<- "New York"
Study18[15:28,3]<-"Connecticut"
Study18[29:42,3]<-"New Jersey"

#Study 63
#5 rows x 4 states 
Study63<- FraudLocation %>% slice(rep(234,20))
Study63[1:5,3] <- "New York"
Study63[6:10,3] <- "Massachusetts"
Study63[11:15,3]<- "Pennsylvania"
Study63[16:20,3]<- "Washington"

#Study 100
#5 rows x 2 states 
Study100<- FraudLocation %>% slice(rep(315,10))
Study100[1:5,3]<- "Lousiana"
Study100[6:10,3] <- "Florida"

#Study 102
#17 rows x 4 states 
Study102<- FraudLocation %>% slice(rep(332,68))
Study102[1:17,3] <- "New York"
Study102[18:34,3]<- "New Jersey"
Study102[35:51,3]<- "Massachusetts"
Study102[52:68,3]<-"Florida"


#Study 121
# 401 rows x 12 states 
Study121<- FraudLocation %>% slice(rep(384,4812))
Study121[1:401,3] <-"Georgia"
Study121[402:802,3]<- "Texas"
Study121[803:1203,3]<-  "Massachusetts"
Study121[1204:1604,3]<- "Illinois"
Study121[1605:2005,3]<- "Colorado"
Study121[2006:2406,3]<- "Missouri"
Study121[2407:2807,3]<- "California"
Study121[2808:3208,3]<- "Florida"
Study121[3209:3609,3]<- "New York"
Study121[3610:4010,3]<- "Pennsylvania"
Study121[4011:4411,3]<- "Oregon"
Study121[4412:4812,3]<- "Washington"


#Study 122
#44 rows x 8 states 
Study122 <- FraudLocation %>% slice(rep(785,352))
Study122[1:44,3]<- "Oregon"
Study122[45:88,3] <- "New York"
Study122[89:132,3]<- "Washington"
Study122[133:176,3]<- "Florida"
Study122[177:220,3]<- "Alabama"
Study122[221:264,3]<- "Mississippi"
Study122[265:308,3]<- "Louisiana"
Study122[309:352,3]<- "Texas"


#Study 123
#34 rows x 2 states
Study123<- FraudLocation %>% slice(rep(829,68))
Study123[1:34,3]<- "Maryland"
Study123[35:68,3]<- "Washington"

#Study 145
#2 rows x 2 states 
Study145<- FraudLocation %>% slice(rep(866,4))
Study145[1:2,3] <- "California"
Study145[3:4,3] <- "New York"

#Study 147
#10 rows x 3 states
Study147<- FraudLocation %>% slice(rep(870,30))
Study147[1:10,3]<- "California"
Study147[11:20,3] <- "Texas"
Study147[21:30, 3] <- "New York"


#Study 150
#1 row x 2 states 
Study150<- FraudLocation %>% slice(rep(887,2))
Study150[1,3]<- "North Carolina"
Study150[2,3] <-  "Louisiana"

#Study 151
#1 row x 2 states 
Study151 <- FraudLocation %>% slice(rep(888,2))
Study151[1,3]<- "Texas"
Study151[2,3] <- "Louisiana"

#Study 154
#1 row x 3 states
Study154<- FraudLocation %>% slice(rep(896,3))
Study154[1,3]<- "Virginia"
Study154[2,3]<- "Delaware"
Study154[3,3]<- "North Carolina"

#Study 160
#1 row x 3 states
Study160<- FraudLocation %>% slice(rep(907,3))
Study160[1,3]<- "Washington"
Study160[2,3]<- "Utah"
Study160[3,3]<- "Texas"

#Removal
FraudLocation <- FraudLocation[-c(1,17,34,35,59:67,79,80,166:183,234:238,315:319,332:348,384:862,866,867,870:872,874:880,887,888,896,907),]

#rbind the studies that were removed 
FraudLocation<-rbind(FraudLocation, Study1, Study3, Study6, Study9, Study10, Study12, 
                     Study17, Study18, Study63, Study100, Study102, Study121, Study122,
                     Study123, Study145, Study147, Study150, Study151, Study154, Study160)

```

I plan to use the FraudLocation data to create a US map showing where most of the samples came from. To do this, I counted each instance where a US state was referenced and created a new data frame called “Location_Count” where I only kept rows that referenced a U.S. state. 

```{r}
#Arrange by ascending order 
FraudLocation<-arrange(FraudLocation, Study.number)

#count the number of instances for each state and make it into its own data frame so I can use that for the US plot
FraudLocation %>% count(Sampling.Area..where.retail.fish.purchased.)->Location_Count
colnames(Location_Count)[1]= "Samping Area"
colnames(Location_Count)[2] = "Total Samples"

#State map 
#remove the rows that don't specify a US state 
Location_USOnly<- Location_Count[-c(5,27,28),]
#rename second column to total
colnames(Location_USOnly)[2]="Total"
colnames(Location_USOnly)[1] = "state"

plot_usmap(data= Location_USOnly, values= "Total", color="black") +
  scale_fill_continuous( 
    low= "white", high= "Blue", name = "Seafood Samples Collected") + 
  theme(legend.position = "right")

```
I next wanted to know the percentage of mislabeled seafood samples. I used the “Number.of.samples” and “Number.of.mislabled.samples” variables and vectored them into objects. Next, I removed all empty strings, NA’s, and “not specified” cells, as well as make sure to turn the strings into integer values so I could divide properly. The percentage of mislabled samples was 26.95%.

```{r}
#turn all the values in the number of samples column into a vector
Total_Samples<- TidyFishFraud$Number.of.samples
# I removed all the empty strings, and all the cases where the number of samples was "not specified". 
Total_Samples<- Total_Samples[Total_Samples!=""]
Total_Samples<- Total_Samples[Total_Samples!=" "]
Total_Samples<-Total_Samples[Total_Samples!="not specified"]
#Turn the strings into integer values
Total_Samples <-Total_Samples %>% strtoi(base=0L) 
#find the sum of the samples, while excluding NAs
Total_Samples<- sum(Total_Samples,na.rm=TRUE)

#turn all values in the mislabeled samples column into a vector
Mislabled_Samples<- TidyFishFraud$Number.of.mislabeled.samples
#remove all empty strings, and all cases where the number of mislabled samples was "not specified"
Mislabled_Samples <-Mislabled_Samples[Mislabled_Samples!=""]
Mislabled_Samples <-Mislabled_Samples[Mislabled_Samples!=" "]
Mislabled_Samples <-Mislabled_Samples[Mislabled_Samples!="not specified"]
Mislabled_Samples<-Mislabled_Samples %>% strtoi(base=0L) 
Mislabled_Samples<- sum(Mislabled_Samples,na.rm=TRUE)

Percentage_Mislabled <- (Mislabled_Samples/Total_Samples)*100


```

In conjunction with this, I wanted to see the percentage of confirmed fish fraud cases using the “Fraud.Mislabeling” variable by counting the total number of “yes” and dividing that by the total number of “no”. I found that 2.14% of fish fraud cases were confirmed as being intentional. Keeping this percentage and the previous one in mind, this tells me that while the confirmed number of intentional fish fraud cases is not that high, the percentage of total mislabeled seafood is. And regardless if it’s done accidentally or not, it still poses consequences to both the consumer and the seafood.   

```{r confirmed fraud}
#turn into vector, only want the fraud mislabeling column
FraudMislabeling_Col<- TidyFishFraud$Fraud.mislabeling.
#count total number of things in vector 
Total_FraudMislabeling<-length(FraudMislabeling_Col)
#only want the cells that contain "yes" and pipe that into the length function
Yes_ConfirmedFraud<- FraudMislabeling_Col[FraudMislabeling_Col=="yes"] %>% length()

Percent_Confirmed <-(Yes_ConfirmedFraud/Total_FraudMislabeling)*100


```

Next, I wanted to know the percentage of seafood labels that match the actual fish being sold. The variables I used were “Seafood.type” and “Species.group”. I created a new data frame called “Match” so then I can use it to create a bar graph to display my results.   

```{r}

Match <- select(TidyFishFraud, Study.number, Study.authors...year,seafood.type, Species.group)
#turn both columns into lowercase
Match$seafood.type <-tolower(Match$seafood.type)
Match$Species.group <-tolower(Match$Species.group)
#now we compare the two columns and create a new column to be added to the match #data frame
Match$Label_Match<- ifelse(Match$seafood.type==Match$Species.group, "Yes",
                           ifelse(Match$seafood.type!= Match$Species.group,"No","NA")) 
#Make into bar chart
BarMatch<-ggplot(Match, aes(Label_Match, fill=Label_Match))
BarMatch+geom_bar()+ 
  labs(title="Match Frequency in Fish Samples", 
       subtitle="How many labels match the fish being sold?")

```

Here we can see that most labels do not match the fish that is being packaged, which further supports the idea that fish mislabeling is a very common occurrence. 

I also wanted to see what were the top 5 seafood that most often gets misbranded. I created a data frame and counted the number of times each seafood was misbranded and manually created a table for the top five most mislabeled fish which looks as follows: 

```{r what are the top 5 seafood groups that get} 

#Take the seafood group column and the label_match column (no), and from there #find the top 5 
Top5<- filter(
  Match, 
  Label_Match=="No"
)
#rename second column 
colnames(Top5)[2]="Total"
#count the total number of seafood types 
Top5 %>% count(seafood.type) ->Top5
#IDK how to get the top5, ive tried 

Top5Temp<- data.frame(seafood.type=c("snapper", "tuna", "caviar,sturgeon", "grouper", "cod"),
                      Total=c(221, 106, 57, 55,33))

```

This is important to note because this seafood is seafood that is commonly sold, meaning that it is more likely that if you purchase any of these fish you may find yourself a victim of fish fraud. 

Next, I wanted to see the IUCN statues of the seafood being sampled. To do this, I made a dot plot using a new data frame called “Conservation” that contains all the conservation data on the samples. Empty strings were replaced with an “Undetermined” label. 

```{r}
#new data frame for the conservation 
Conservation<- select(
  TidyFishFraud, 
  Study.number, 
  IUCN.Conservation.Status
)
#replace empty strings with "undetermined" 
Conservation[Conservation ==""] <- "undetermined"
Conservation %>% count(IUCN.Conservation.Status) -> ConservationCount
#rename column to total 
colnames(ConservationCount)[2]="Total"
#make dot plot 
ggplot(ConservationCount, aes(x=IUCN.Conservation.Status, y=Total)) + 
  geom_point(col="tomato2", size=3)+
  geom_segment(aes(x=IUCN.Conservation.Status,
                   xend=IUCN.Conservation.Status,
                   y=min(Total),
                   yend=max(Total)),
               linetype="dashed", 
               size=0.1) +
  labs(title='IUCN Conservation Status of the Samples',
       caption="Key: NT = Near Threatened; VU = Vulnerable; EN=Endangered; 
       CR= Critically Endangered; NE= Not Evaluated; LS = Least Concern; 
       DD = Data Deficient; Undetermined = Status was not designated")

```

This is significant because it shows us that many of the fish being mislabeled are in some way endangered. Mislabeling for the sake of profit could pose a threat to cheaper fish as they could potentially be overfished and damage not only the organisms but also the ecosystems that they thrive in.


To add to the consequences of fish fraud, I made a bar chart showing the health risks associated with consuming the fish being packaged, regardless of what is mentioned on the label. I turned the “Health.Risks” variable into the “Risks” data frame and turned empty strings into “undetermined” and NA’s into “No Risk” as if there was a risk, it would have been documented. Within this data frame, there were a lot of rows that mentioned 2+ health risks, so I turned these into “Combination” health risks to show that there can be more than one health risk associated with consuming certain seafood.

```{r}
#data frame for risks to then make it into an ordered bar chart 
Risks<- select(TidyFishFraud, Study.number, Health.Risk)
#empty strings turn into "undetermined"
Risks[Risks==""]<-"undetermined"
#na's will be "No Risk"
Risks[Risks=="na"]<- "No Risk"
#Any cell that has 2+ risks will turn into "Combination"
Risks[str_detect(Risks, pattern = " [+]* ")]
Risks[Risks=="NT+P+H+M"]<-"Combination"
Risks[Risks=="P+EC"]<- "Combination"
Risks[Risks=="H+NT"]<- "Combination"
Risks[Risks=="EC+AqD"]<- "Combination"
Risks[Risks=="EC+AqD+P"]<- "Combination"
Risks[Risks=="EC+AqD"]<- "Combination"
Risks[Risks=="Ec+AqD+NT+H"]<-"Combination"
Risks[Risks=="M+H"]<- "Combination"
Risks[Risks=="M+H+P"]<- "Combination"
Risks[Risks=="M+H+P+NT+EC"]<- "Combination"
Risks[Risks=="P+NT+EC"]<- "Combination"
Risks[Risks=="P+NT"]<- "Combination"
Risks[Risks=="P+H"]<- "Combination"
Risks[Risks=="P+EC+AqD"]<- "Combination"
Risks[Risks=="NT+H"]<- "Combination"
Risks[Risks=="M+P"]<- "Combination"
Risks[Risks=="EC+P"]<- "Combination"
#count
Risks %>%  count(Health.Risk) -> Risks2
#rename column 2
colnames(Risks2)[2]<- "Total"
#bar graph time 
ggplot(Risks2, aes(x=Health.Risk, y=Total)) + 
  geom_bar(stat="identity", width=.5, fill= "tomato3") + 
  labs( title= 'Health Risks Identified in Mislabeled Samples',
        caption= "H = Histamine ; P = Parasites; NT = Natural Toxins; M = Mercury; 
        EC = Environmental Chemicals; AqD= Aquaculture Drugs ")


```

Health risks associated with consuming some of the fish in the samples can range from allergic reactions (Histamine), risk of parasites, natural toxins, Mercury, and/or environmental chemicals, not to mention the risk of consuming trace amounts of Aquaculture drugs which are antibiotics given to sea life to treat certain bacterial or fungal diseases. This graph shows us that most of the health risks that one might encounter could likely be a combination of two or more of the aforementioned health risks. The reaction and severity of it will vary from person to person but there is no denying the fact that it is especially unsafe to consume seafood when we can’t be sure if it's the actual seafood we think we’re eating.


My final question concerned figuring out where most of the samples were acquired from along the supply chain. I wanted to demonstrate this in the form of a tree map. I created the TreeData data frame with the “Point.in.supply.chain” variable and replaced empty strings with “undetermined” so it can be a category in the tree map. There were a lot of instances where 2+ locations were referenced in a single cell, so I will repeat the same process as I did with the data in the US map and repeat rows and bind() at the end. 
```{r}

TreeData<- select(TidyFishFraud, Study.number, Study.authors...year, Point.in.supply.chain)
# replace the empty strings with "undetermined" as a category
TreeData[TreeData==''] <- "undetermined"
TreeData[TreeData=="retail"]<-"Retail (undetermined)"

#Study 15 
#85 rows x 3 location = 255 repeats
T_Study15<-TreeData %>% slice(rep(81,255))
T_Study15[1:85, 3]<- "retail, grocery"
T_Study15[86:170, 3]<- "retail, restuarant"
T_Study15[171:255, 3]<- "retail, market"

#Study 18 
T_Study18 <- TreeData %>% slice(rep(170,28))
T_Study18[1:14,3] <- "retail, grocery"
T_Study18[15:28,3] <- "retail, restaurant"

#Study 20 
T_Study20<- TreeData %>% slice(rep(185, 22))
T_Study20[1:11,3]<- "retail, grocery"
T_Study20[12:22,3] <- "retail, restaurant"

#Study 66
T_Study66<- TreeData %>% slice(rep(239,4))
T_Study66[1:2,3]<- "Retail (undetermined)"
T_Study66[3:4,3] <- "Wholesale"

#Study85 
T_Study85<- TreeData %>% slice(rep(266,2))
T_Study85[1,3]<- "Retail (undetermined)"
T_Study85[2,3]<- "Wholesale"

#Study 86 
T_Study86 <- TreeData %>% slice(rep(267,52))
T_Study86[1:26,3]<- "retail"
T_Study86[27:52,3]<- "import"

#Study 91
T_Study91 <- TreeData %>% slice(rep(296,2))
T_Study91[1,3]<- "retail, grocery"
T_Study91[2,3]<- "distributor"

#Study 97 
T_Study97 <- TreeData %>% slice(rep(298, 2))
T_Study97[1,3] <- "retail, grocery"
T_Study97[2,3] <- "retail, restaurant"

#Study 109
T_Study109<- TreeData %>% slice(rep(369,4))
T_Study109[1:2,3]<- "distributor"
T_Study109[3:4,3]<- "import"

#Study147
T_Study147<- TreeData %>% slice(rep(873,2))
T_Study147[1,3]<- "Retail (undetermined)"
T_Study147[2,3]<- "Wholesale"

#Study 161
T_Study161<- TreeData %>% slice(rep(908,2))
T_Study161[1,3]<- "Retail (undetermined)"
T_Study161[2,3]<- "Export"

#Study 163 
T_Study163 <-TreeData %>% slice(rep(911,2))
T_Study163[1,3]<- "Retail (undetermined)"
T_Study163[2,3] <- "Wholesale"

#Study 168 
T_Study168<- TreeData %>% slice(rep(918,2))
T_Study168[1,3]<- "Retail / Fish Dealer"
T_Study168[2,3]<- "Wholesaler"

#I am now going to remove all the studies that have 2+ point in supply chain locations 
TreeData<- TreeData[-c(81:165,170:183,185:195,239,240,266:292,296,298,369,370,873,911,908,918),]

#rbind all the studies to the TreeData df 
TreeData<- rbind(TreeData, T_Study15, T_Study18, T_Study20, T_Study66, T_Study85, T_Study86,
                 T_Study91, T_Study97, T_Study109, T_Study147, T_Study161, T_Study163, T_Study168)

```

Some NA’s remain in the study number and author columns but that won’t impact our result. I realized I needed a second column to serve as my subgroup for the tree map. So, I duplicated the “Point.in.supply.chain” column and renamed it as the “SC Sub Group” column. The “Point.in.supply.chain” column was renamed as “Point in SC.” “Point in SC” will have the general location such as retail, import, export, and distributor. While “SC Sub Group” will have more niche locations such as grocery, market, fish dealers, restaurants, etc. 

 After I used rbind() to join the T-study data frames, I found that the TreeData data frame had its indexing out of order so I made sure to order it again by ascending order. That way, I could begin to change the cells in the “Point in SC” column to reflect the general locations and change “SC Sub Group” to reflect the niche locations. I also added a value column filled with 1’s so it can be used in the “vSize” variable when creating the treemap. 

```{r}

#duplicate the point in supply chain column so i can alter the second column easier
TreeData$SC.Sub.Group<- TreeData$Point.in.supply.chain
colnames(TreeData)[3]="Point in SC"
colnames(TreeData)[4]= "SC Sub Group"

#lowercase to uppercase 
TreeData[TreeData == "undetermined"] <- "Undetermined"
TreeData[TreeData== "retail"]<- "Retail"
TreeData[TreeData=="import"]<- "Import"
TreeData[TreeData=="importer"]<- "Import"
TreeData[TreeData=="restaurant"]<- "Restaurant"
TreeData[TreeData=="distributor"]<- "Distributor"
TreeData[TreeData=="wholesale"]<- "Wholesale"
TreeData[TreeData=="packager"]<- "Packager"
TreeData[TreeData=="shipping"]<- "Shipping"
TreeData[TreeData=="processor"]<- "Processor"

#order the row index numbers
row.names(TreeData)<- NULL

TreeData[1,3]<- "Retail"
TreeData[1,4]<-"Undetermined"
TreeData[17:19,3]<- "Retail"
TreeData[17:19,4]<- "Undetermined"
TreeData[20:64,3]<- "Retail"
TreeData[20:32,4]<- "Fish Dealer"
TreeData[33:64,4]<- "Undetermined"
TreeData[68:85,3]<- "Retail"
TreeData[68:85,4]<- "Undetermined"
TreeData[88:151,3]<- "Retail"
TreeData[88:119,4]<- "Undetermined"
TreeData[120:123,4]<- "Restaurant"
TreeData[124:136,4]<- "Undetermined"
TreeData[137:141,4]<- "Restaurant"
TreeData[142:150,4]<- "Market"
TreeData[151,4]<- "Restaurant"
TreeData[156:157,3]<- "Retail"
TreeData[156:157,4]<- "Undetermined"
TreeData[161:227,3]<- "Retail"
TreeData[161:227,4]<- "Undetermined"
TreeData[235:239,3]<- "Retail"
TreeData[235:239,4]<- "Restaurant"
TreeData[241:722,3]<- "Retail"
TreeData[241:722,4]<- "Undetermined"
TreeData[723:736,3]<- "Retail"
TreeData[741:742,3]<- "Retail"
TreeData[770:771,3]<- "Retail"
TreeData[770:771,4]<- "Undetermined"
TreeData[773:796,3]<- "Retail"
TreeData[780,4]<- "Undetermined"
TreeData[783:788,4]<- "Undetermined"
TreeData[789:794,4]<- "Restaurant"
TreeData[795:796,4]<- "Undetermined"
TreeData[798,3]<- "Retail"
TreeData[831:1137,3]<- "Retail"
TreeData[831:915,4]<- "Grocery"
TreeData[916:1000,4]<- "Restaurant"
TreeData[1001:1085,4]<- "Market"
TreeData[1086:1099,4]<- "Grocery"
TreeData[1100:1113,4]<- "Restaurant"
TreeData[1114:1124,4]<- "Grocery"
TreeData[1125:1135,4]<- "Restaurant"
TreeData[1136:1137,4]<- "Undetermined"
TreeData[1140,3]<- "Retail"
TreeData[1140,4]<- "Undetermined"
TreeData[1194,3]<- "Retail"
TreeData[1194,4]<- "Grocery"
TreeData[1196:1197,3]<- "Retail"
TreeData[1196,4]<- "Grocery"
TreeData[1197,4]<- "Restaurant"
TreeData[1202,3]<- "Retail"
TreeData[1202,4]<- "Undetermined"
TreeData[1204,3]<- "Retail"
TreeData[1204,4]<- "Undetermined"
TreeData[1206,3]<- "Retail"
TreeData[1206,4]<- "Undetermined"
TreeData[1208,3]<- "Retail"
TreeData[1208,4]<- "Fish Dealer"
TreeData[1142:1167,4]<- "Undetermined"

#add numerical column 
TreeData$Value<- 1

treemap(TreeData,
        index= c("Point in SC","SC Sub Group"),
        vSize= "Value",
        type= "index",
        title= "Location of the Samples Acquired: Supply Chain",
        fontsize.title = 14
        )

```

Based on this treemap we can see that most of the samples were found in retail with few samples being from distributors, importers, and other locations. Within the scope of fish fraud, this means that most instances of mislabeling would be found within retail, likely because by that point someone already has their hands on the seafood and has the opportunity to see if the label matches the sold seafood. Throughout most of the other locations across the supply chain, the seafood is constantly being moved, so there may not be many opportunities or interest to double-check if everything is being represented properly as the primary goal is to get the seafood to its intended destination. 

# Summary 

Fish fraud is an important issue that is affecting all of us without realizing it and it's important for us to be aware of this the next time we consume or purchase any seafood. The aim of this project was to inform seafood consumers by addressing the following questions:

- How common is seafood fraud?
- What are the risks/consequences associated with seafood fraud?

Based on my findings I can say that while cases of confirmed, intentional, fish fraud may not be that frequent (2.14%) cases of mislabeling are (26.95%) as further supported by the fact that most seafood do not match what is presented on the label. This is important to note as a consumer because it doesn’t matter if the mislabeling is intentional or not because at the end of the day it can pose health risks for its consumers. Health risks such as exposure to Mercury, environmental toxins, parasites, and the possibility of an allergic reaction. There are also threats to the seafood themselves as it can negatively impact the seafood population due to overfishing or can have a negative impact on its environment. 

Some limitations that I encountered were gaps in the original data that I received from Oceana. There were a lot of instances that I had to remove empty strings or NA’s within columns that should have otherwise contained important data that would have been beneficial to my study. Additionally, I suspect that there is some sampling bias present, although likely not intentional, because it would be difficult to determine and know where to look for instances of fish fraud or mislabeling. It is likely that the data I have is not fully representative of the issue on a wider scale. Lastly, the limited access to data made it hard to get a full scope of the issue as most of the data related to fish fraud is behind a pay wall. Had I not had this restriction I would have loved to compare this data with other similar data sets to get a better idea of what fish fraud looks like in different regions. 
